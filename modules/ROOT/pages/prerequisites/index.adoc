= Prerequisites
:toc: right
:toclevels: 2

:ext4_url: https://en.wikipedia.org/wiki/Ext4
:btrfs_url: https://en.wikipedia.org/wiki/Btrfs
:zfs_url: https://en.wikipedia.org/wiki/ZFS
:xfs_url: https://en.wikipedia.org/wiki/XFS
:cephfs_url: https://en.wikipedia.org/wiki/Ceph_(software)#File_system
:nfs_url: https://en.wikipedia.org/wiki/Network_File_System

:nginx-url: https://docs.nginx.com/nginx/admin-guide/web-server/reverse-proxy/
:traefik-url: https://doc.traefik.io/traefik/
:apache-rev-url: https://httpd.apache.org/docs/2.4/howto/reverse_proxy.html

:description: The prerequisites section gives an overview and background about minimum requirements with respect to hardware, virtualization, operating system and optional software to operate oCIS successfully.

== Introduction

{description}

Note that oCIS is based on a highly dynamic interoperable _microservices_ concept, although it is delivered as a single binary. Of course, this software has to run on hardware, but its ability to adapt to various and maybe dynamic load scenarios across physical boundaries means that you can set up your physical environment flexibly, using modern technology.

The following sections are fundamental to understanding the general prerequisites, but their implementation is very dependent on your use-case. To support your decision about the environment, we recommend reading the documents about  xref:architecture/index.adoc[Architecture and Concepts] and xref:availability_scaling/index.adoc[Availability and Scalability] first.

== Hardware

The minimum hardware requirements depend on the usage scenario you have in mind for your oCIS instance. You need at least hardware that runs a Linux based operating system, preferably 64bit.

* For simple testing purposes, you can even use a laptop or Raspberry Pi 4 (ARM64) with a minimum of 512MB of RAM.
* For more intense testing purposes, the recommended start hardware is a multi-core processor with at least 4GB of RAM.
* For production environments, CPU and RAM are the main limiting factors. A clear guideline for CPUs can't be provided at this time since there are too many influencing factors. See xref:ram-considerations[RAM Considerations] for memory and xref:bandwidth-considerations[Bandwidth Considerations] for network-related factors.

// fixme: info of architectures came from willy, also see: 
// https://download.owncloud.com/ocis/ocis/1.18.0/
// https://hub.docker.com/r/owncloud/ocis/tags

The following table shows the tested and supported hardware architecture matrix::
[width="65%",cols="50%,50%",options="header"]
|===
| OS / Docker
| Hardware Architecture

| Linux on Bare Metal
| 386, AMD64, ARM, ARM64

| Linux Hardware Virtualized
| 386, AMD64, ARM, ARM64

| Linux Docker
| AMD64, ARM, ARM64

| Darwin (MacOS)
| AMD64, ARM64, +
M1 (ARM64 compatible)
|===

Note when referencing software builds for a particular architecture, *ARM* stands for ARM32(v6/v7) and *ARM64* stands for ARM64(v8).

=== RAM Considerations

// harvested from https://owncloud.dev/architecture/efficient-stat-polling/
// text adopted based on an intense discussion with jfd/willy on 11.3.

// fixme: how to read stat info (local and eg nfs)

RAM requirements are mainly based on caching the mounted filesystems metadata like inodes (even extended attributes are supported) and include calculated ETags (entity tag). metadata caching is mostly based on the kernel caching capability of the OS for the mounted filesystems. Dependent on the filesystem(s) mounted, a different block size for the metadata may be used. Avoid swap space usage because of metadata caching for performance reasons.

NOTE: In an ideal environment, all the metadata is kept in memory for best performance. If that information is not available in memory, physical access to the storage backend is necessary to gather and calculate the required information, which slows down the response. In real life, memory for cached metadata, physical storage access and performance will have to be balanced.

NOTE: Your RAM requirements may change over time depending on the amount of users accessing an increasing amount of data from different sources with different change rates concurrently.

==== Calculation example:

For a single machine using a local posix storage, the Linux kernel caches metadata which is necessary to calculate the ETag. Note that the following description focuses on classical metadata. Additional information added to the metadata by oCIS like user added tags or comments are not considered. This functionality may increase the metadata quantity that should be cached.

// fixme: different fs have different metadata block-sizes or can even chain them

With a metadata size of `4K` for each block, as is used in the EXT4 filesystem (see xref:filesystems-and-shared-storage[Filesystems and Shared Storage] for other values), the following RAM is needed to completely cache all metadata:

[width="60%",cols="^40%,^70%",options="header"]
|===
| Metadata items
a| RAM requirement for [.underline]#_caching only_#

| 256
| 1 MB

| 1 K
| 4 MB

| 1 M
| 4 GB
|===

Using the above table and some sample environments from below, you can easily match the sweet spot and possibly extrapolate it to your environment. Consider that more users will do more requests for content or changes regularly, independent of whether they are accessing oCIS via the web interface, via a mobile client or the desktop synchronization client. If this access is not held in memory, the access needs to go through the backend slowing down the overall response and consumes more CPU accordingly.

Note that the table below shows only one xref:background[spaces] created and no shares or re-shares which would multiply the needed quantity of metadata.

[[sample_environments]]
[caption=]
.Sample environments (Node = file or folder)
[width="80%",cols="75%,^65%,^65%,^65%",options="header"]
|===
|
| Environment 1
| Environment 2
| Environment 3

a| *Nodes*
| 3 Millions
| 2 Millions
| 3 Billions

a| *Users*
| 30
| 80
| 500

a| *Changed Nodes/h*
| 65
| 18
| 178

a| *Changed meta/h*
| 260
| 72
| 712
|===

It is assumed in the examples above, that on avarage, the original change was 4 levels below the root level of a single spaces, creating 4 times the quantity of changed metadata.

==== Background

Storing metadata in memory is important with respect to access and synchronization performance. oCIS has the concept of _spaces_ which you can see in a nutshell as a space containing files and folders which is not owned by a user by default, but gets assigned to users. Spaces can be the users home, any mount created or an unlimited quantity of shares and re-shares.

Each spaces provides a virtual root and occupies at least one metadata block at its root. Additionally each spaces provides the complete metadata information about the underlying content. As you can see, the quantity of metadata can grow large.

Whenever a file or folder (node) changes its content or metadata somewhere in the path, the ETag gets recalculated and the change is propagated to the spaces root for identifying a change. To let a client detect changes, the discovery phase starts at the spaces root and checks if the ETag has changed since the last discovery going forward in the tree of changed ETags.

Backend Check::
Checks for changes in the backend are made every 30 seconds for the available spaces. As spaces are owner-less, the check appears only once in that interval. The value is configurable to adapt to storage IO load.

Client Check::
Usually, every connected client also polls his assigned spaces root ETag every 30 seconds and compares it to the former ETag received for changes. Based on detected changes, actions take place.

This makes it clear why RAM is an essential performance factor for client access and synchronization when more spaces are present. If the gathered spaces root ETag has changed and the underlying metadata is not in memory, it has to be queried from the physical storage and the ETag needs recalculation. 

// fixme: how to get the actual cache % for a server
// fixme: where to tune in case (vfs_cache_pressure ?) or is this not wanted
// fixme: see https://rudd-o.com/linux-and-free-software/tales-from-responsivenessland-why-linux-feels-slow-and-how-to-fix-that
// fixme: https://manhart.blog/2020/04/linux-leistungsverbesserungen/
// fixme: personal note: reducing vfs_cache_pressure from 100 to 20 improved the "real" component by 40% up to 90%!

=== Bandwidth Considerations

The bandwidth requirements and limitations are based on the following background. Note that this is a view on the internal network (LAN) only. Any considerations about access from the Internet are not part of this description but can be derived from the LAN point of view:

Clients, which are accessing oCIS, request information about what has changed. Depending on the response and if a file synchronization is required, different bandwidth needs may result. Note that when using e.g. the Desktop Client and virtual files (VFS), only those files get physically synced which are set to be locally present, preventing additional bandwidth consumption.

// fixme: the bandwidth calculation in the devdocs is imho wrong as the bigger number is the response and not the request which is then the delimiting factor

Request for changed elements::
To get the information about changes, the request always starts at the spaces root, looking for changed ETags, and follows only a path that has changed elements. Therefore PROPFIND requests and responses are used. A request has about 500 bytes and a response has roughly 800 bytes in size.
+
[caption=]
.Number of maximum concurrent PROPFIND responses per second
[width="60%",cols="60%,80%",options="header"]
|===
| Network
| max. PROPFIND responses/s

| 100 Mbit (~10MB/s)
| 12.500

| 1 Gbit (~100MB/s)
| 125.000

| 10 Gbit (~1GB/s)
| 1.250.000
|===

Request syncing changed files::
When a file has been identified to be physically synced, the bandwidth requirements depend on the size and the time it should finish. Note that syncing changed files can saturate a network more easily than the handling of changed ETags!

Calculation example::
// fixme: check calculation
+
Consider the xref:sample_environments[sample environments table] above to see how clients will trigger PROPFIND responses resulting in bandwidth consumption. 500 concurring syncing users, syncing with the default setting of every 30 sec, will create about ~3K PROPFIND requests (500 x 712 / 60 / 2) which consume about 2.4MB/s of bandwidth (3K x 800B) - without doing the file syncs necessary. The physical transfer will create extra bandwidth requirements.

Summary::
[NOTE]
====
As you can see above, the bandwidth requirements depend on:

* The number of concurrent clients accessing oCIS
* The quantity of files and folders
* The dynamics of changes
* The relative location of the change
* The need to download changed files locally
====
// fixme: to be clarified about scaling
// NOTE: Bandwidth can be scaled by adding more gateways and distributing users between them because these components are stateless.

== Virtualization

Depending on the usecase, you can run oCIS on:

* No virtualization, bare metal
* Virtualized hardware like VMWare, KVM, HyperV, VirtualBox etc.
* Virtualized Linux operating system in Docker containers

== Supported Operating Systems

For _best performance_, _stability_, _support_, and _full functionality_ we officially support oCIS running on the following Linux distributions:

* Debian 10 and 11
* Fedora 32 and 33
* Red Hat Enterprise Linux/Centos 7.5 and 8
* SUSE Linux Enterprise Server 12 with SP4/5 and SLES 15
* openSUSE Leap 15.2 and 15.3
* Ubuntu 20.04

== Additional Software

It is strongly recommend to use a reverse proxy like {traefik-url}[Traefik], {nginx-url}[NGINX] or {apache-rev-url}[Apache] for:

. security reasons,
. load balancing and
. high availability.

// fixme: describe the reason for the need
// fixme: links to how to setup these things, maybe external links will work well too

== Filesystems and Shared Storage

In addition to well known metadata like _name_, _size_ and _mtime_ (time a file was last modified), oCIS allows users to add arbitrary metadata like _tags_ and _comments_. Therefore oCIS requires and works with POSIX-compliant file systems where this metadata can be mapped to extended attributes. S3 storage is supported too, but requires POSIX-compliant local storage for metadata persistance.

The currently supported oCIS POSIX-compliant file systems are the following. Note that the default block size equals the metadata block size used for the xref:ram-considerations[RAM Considerations], which is on some filesystems definable and if applicable, is for informational purposes only:

[caption=]
.Local Filesystems
[width="60%",cols="30%,70%",options="header"]
|===
| Name
| Default Block Size

a| {ext4_url}[EXT4]
| 4K

a| {xfs_url}[XFS]
| 4K

a| {btrfs_url}[BTRFS]
| 16K

a| {zfs_url}[ZFS]
| 128K
|===

[caption=]
.Shared Filesystems
[width="80%",cols="26%,90%",options="header"]
|===
| Name
| Default Block Size

a| {nfs_url}[NFS]
| The block size depends on the `rsize` parameter in the mount options. Defaults to 4K, usually set to 32K.

a| {cephfs_url}[CephFS]
| CephFS does not have metadata by design, You have to use a POSIX-compliant filesystem (see list above) to store the metadata.
|===

[NOTE]
====
When using NFS, you have to take care that the NFS server provides `Extended Attributes`.

NFS storage based on Linux servers::
Extended attributes are supported by NFS starting with Kernel version 5.9, which means that the server with the NFS server has to run a kernel with that or a higher version number. To check that, run the command `uname -a` on the NFS server and compare the displayed version number.
+
Note that if the kernel supports extended attributes, you have to use NFSv4 in order to use it.

NFS servers provided from storage vendors::
A certification matrix will be provided when available.
====
