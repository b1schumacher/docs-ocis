= Prerequisites
:toc: right
:toclevels: 2

:ext4_url: https://en.wikipedia.org/wiki/Ext4
:btrfs_url: https://en.wikipedia.org/wiki/Btrfs
:zfs_url: https://en.wikipedia.org/wiki/ZFS
:xfs_url: https://en.wikipedia.org/wiki/XFS
:cephfs_url: https://en.wikipedia.org/wiki/Ceph_(software)#File_system
:nfs_url: https://en.wikipedia.org/wiki/Network_File_System

:nginx-url: https://docs.nginx.com/nginx/admin-guide/web-server/reverse-proxy/
:traefik-url: https://doc.traefik.io/traefik/
:apache-rev-url: https://httpd.apache.org/docs/2.4/howto/reverse_proxy.html

:description: The prerequisites section gives an overview and background about minimum requirements with respect to hardware, virtualization, operating system and optional software to operate oCIS successfully.

== Introduction

{description}

Note that oCIS is based on a highly dynamic interoperable _microservices_ concept, even when it is delivered as single binary. Of course this software has to run on hardware, but its ability to adapt to various and maybe dynamic load scenarios across physical boundaries means that you can setup your physical environment flexibly using modern technology.

The following sections are fundamental to understanding the general prerequisites, but their implementation is very dependent on your use-case. To support your decision about the environment, we recommend reading the documents about  xref:architecture/index.adoc[Architecture and Concepts] and xref:availability/index.adoc[Availability, Redundancy and Load Distribution] first.

== Hardware

The minimum hardware requirements depend on the usage scenario you have in mind for your oCIS instance. You need at least hardware that runs a Linux based operating system, preferably 64bit.

* For simple testing purposes, you can even use a laptop or Raspberry Pi 4 (ARM64) with a minimum of 512MB of RAM.
* For more intense testing purposes, the recommended start hardware is a multi-core processor with at least 4GB of RAM.
* For production environments, CPU and RAM are the main limiting factors. A clear guideline for CPU's cant be given at the moment as there are too many influencing factors. See xref:ram-considerations[RAM Considerations] for memory and xref:bandwidth-considerations[Bandwidth Considerations] for network related factors.

// fixme: info of architectures came from willy, also see: 
// https://download.owncloud.com/ocis/ocis/1.18.0/
// https://hub.docker.com/r/owncloud/ocis/tags

The following table shows the tested and supported hardware architecture matrix::
[width="65%",cols="50%,50%",options="header"]
|===
| OS / Docker
| Hardware Architecture

| Linux on Bare Metal
| 386, AMD64, ARM, ARM64

| Linux Hardware Virtualized
| 386, AMD64, ARM, ARM64

| Linux Docker
| AMD64, ARM, ARM64

| Darwin (MacOS)
| AMD64, ARM64, +
M1 (ARM64 compatible)
|===

Note when referencing software builds for a particular architecture, *ARM* stands for ARM32(v6/v7) and *ARM64* stands for ARM64(v8).

=== RAM Considerations

// harvested from https://owncloud.dev/architecture/efficient-stat-polling/
// text adopted based on an intense discussion with jfd/willy on 11.3.

// fixme: how to read stat info (local and eg nfs)

RAM requirements are mainly based on caching the mounted filesystems meta-data like inodes (even extended attributes are supported) and include calculated ETags (entity tag). Meta-data caching is mostly based on the kernel caching capability of the OS for the mounted filesystems. Dependent on the filesystem(s) mounted, a different block size for the meta-data may be used. Avoid swap space usage because of meta-data caching for performance reasons.

NOTE: In an optimal environment, all the meta-data is kept in memory for best perfomance. If that information is not available in memory, physical access to the storage backend needs to be made to gather and calculate the required information slowing down the response. In real life, a balance between memory for cached meta-data, physical storage access and performance will take place.

NOTE: Your RAM requirements may change over time depending on the amount of users accessing an increasing amount of data from different sources with different change rates concurrently.

==== Calculation example:

For a single machine using a local posix storage, the Linux kernel caches meta-data which is necessary to calculate the ETag. Note that the following description focuses on classical meta-data. Additional information added to the meta-data by oCIS like user added tags or comments are not considered. This functionality may increase the meta-data quantity that should be cached.

// fixme: different fs have different meta-data block-sizes or can even chain them

With a meta-data size of 4K each, as is used in the EXT4 filesystem, the following RAM is needed to completely cache all meta-data:

[width="60%",cols="^40%,^70%",options="header"]
|===
| Meta-data items
a| RAM requirement for [.underline]#_caching only_#

| 256
| 1 MB

| 1 K
| 4 MB

| 1 M
| 4 GB
|===

Using the above table and some sample environments from below, you can easily match the sweet spot and possibly extrapolate it to your environment. Consider that more users will do more requests for content or changes regularly, independent of if they are accessing oCIS via the web interface, via a mobile client, or the desktop synchronisation client. If this access is not held in memory, the access needs to go thru the backend slowing down the overall response and consumes more CPU accordingly.

Note that the table below shows only one xref:background[spaces] created and no shares or reshares which would multiply the needed quantity of meta-data.

[[sample_environments]]
[caption=]
.Sample environments (Node = file or folder)
[width="100%",cols="70%,^40%,70%,^40%,70%,^40%",options="header"]
|===
2+^| Environment 1
2+^| Environment 2
2+^| Environment 3

a| *Nodes*
| 3 Millions
a| *Nodes*
| 2 Millions
a| *Nodes*
| 3 Billions

a| *Users*
| 30
a| *Users*
| 80
a| *Users*
| 500

a| *Changed Nodes/h*
| 65
a| *Changed Nodes/h*
| 18
a| *Changed Nodes/h*
| 178

a| *Changed meta/h*
| 260
a| *Changed meta/h*
| 72
a| *Changed meta/h*
| 712
|===
It is assumed in the examples above, that on avarage, the original change was 4 levels below the root level of a single space, creating 4 times the quantity of changed meta-data.

==== Background

Storing meta-data in memory is important with respect to access and synchronisation performance. oCIS has the concept of _spaces_ which you can see in a nutshell as a space containing files and folders which is not owned by a user by default, but gets assigned to users. Spaces can be the users home, any mount created or an unlimited quantity of shares and reshares.

Each space provides a virtual root and occupies 4K for this root meta-data. Additionally each space provides the complete meta-data information about the underlying content. As you can see, the quantity of meta-data can grow large.

Whenever a file or folder (node) changes its content or metadata somewhere in the path, the ETag gets recalculated and the change is propagated to the spaces root for identifying a change. To let a client detect changes, the discovery phase starts at the spaces root and checks if the ETag has changed since the last discovery going forward in the tree of changed ETags.

Backend Check::
Checks for changes in the backend are made every 30 seconds for the available spaces. As the space is owner-less, the check appears only once in that interval. The value is configurable to adapt to storage IO load.

Client Check::
Usually, every connected client polls his assigned spaces root ETag also every 30 seconds and compares to the former ETag recieved for changes. Based on detected changes, actions take place.

This makes it clear why RAM is an essential performance factor for client access and synchronisation when more spaces are present. If the gathered spaces root ETag has changed and the underlying meta-data is not in memory, it has to be queried from the physical storage and the ETag needs recalculation. 

// fixme: how to get the actual cache % for a server
// fixme: where to tune in case (vfs_cache_pressure ?) or is this not wanted
// fixme: see https://rudd-o.com/linux-and-free-software/tales-from-responsivenessland-why-linux-feels-slow-and-how-to-fix-that
// fixme: https://manhart.blog/2020/04/linux-leistungsverbesserungen/
// fixme: personal note: reducing vfs_cache_pressure from 100 to 20 improved the "real" component by 40% up to 90%!

=== Bandwidth Considerations

The bandwidth requirements and limitations are based on the following background. Note that this is a view on the internal network (LAN) only. Any considerations about access from the Internet are not part of this description but can be derived from the LAN point of view:

Clients, which are accessing oCIS, request information about what has changed. Depending on the response and if a file synchronization is required, different bandwidth needs may result. Note that when using e.g. the Desktop Client and virtual files (VFS), only those files get physically synced which are set to be locally present, preventing additional bandwidth consumption.

// fixme: the bandwidth calculation in the devdocs is imho wrong as the bigger number is the response and not the request which is then the delimiting factor

Request for changed elements::
To get the information about changes, the request always starts at the spaces root looking for changed ETags and goes along only that path that has changed elements. Therefore PROPFIND requests and responses are used. A request has about 500 bytes and a response has roughly 800 bytes in size.
+
[caption=]
.Number of maximum concurrent PROPFIND responses per second
[width="60%",cols="55%,80%",options="header"]
|===
| Network
| max. PROPFIND responses/s

| 100 Mbit (~10MB/s)
| 12.500

| 1 Gbit (~100MB/s)
| 125.000

| 10 Gbit (~1GB/s)
| 1.250.000
|===

Request syncing changed files::
When a file has been identified to be physically synced, the bandwidth requirements depend on the size and the time it should finish. Note that syncing changed files can saturate a network more easily than the handling of changed ETags!

Calculation example::
// fixme: check calculation
+
Consider the xref:sample_environments[sample environments table] above to see how clients will trigger propfind responses resulting in bandwidth consumption. 500 concurring syncing users, syncing with the default setting of every 30 sec, will create about ~3K propfind requests (500 x 712 / 60 / 2) which consume about 2.4MB/s of bandwidth (3K x 800B) - without doing the file syncs necessary. The physical transfer will create extra bandwidth requirements.

Summary::
[NOTE]
====
As you can see above, the bandwidth requirements depend on:

* The number of concurrent clients accessing oCIS
* The quantity of files and folders
* The dynamics of changes
* The relative location of the change
* The need to download changed files locally
====
// fixme: to be clarified about scaling
// NOTE: Bandwidth can be scaled by adding more gateways and distributing users between them because these components are stateless.

== Virtualization

Depending on the usecase, you can run oCIS on:

* No virtualization, bare metal
* Virtualized hardware like VMWare, KVM, HyperV, VirtualBox etc.
* Virtualized Linux operating system in Docker containers

== Supported Operating Systems

For _best performance_, _stability_, _support_, and _full functionality_ we officially support oCIS running on the following Linux distributions:

* Debian 10 and 11
* Fedora 32 and 33
* Red Hat Enterprise Linux/Centos 7.5 and 8
* SUSE Linux Enterprise Server 12 with SP4/5 and SLES 15
* openSUSE Leap 15.2 and 15.3
* Ubuntu 20.04

== Additional Software

It is strongly recommend to use a reverse proxy like {traefik-url}[Traefik], {nginx-url}[NGINX] or {apache-rev-url}[Apache] for:

. security reasons,
. load balancing and
. high availability.

// fixme: describe the reason for the need
// fixme: links to how to setup these things, maybe external links will work well too

== Filesystems and Shared Storage

In addition to well known metadata like _name_, _size_ and _mtime_ (time a file was last modified), oCIS allows users to add arbitrary metadata like _tags_ and _comments_. Therefore oCIS requires and works with POSIX-compliant file systems where this metadata can be mapped to extended attributes. S3 storage is supported too, but requires POSIX-compliant local storage for meta data persistance.

The currently supported oCIS POSIX-compliant file systems are:

. Local Filesystems
* {btrfs_url}[BTRFS]
* {ext4_url}[EXT4]
* {xfs_url}[XFS]
* {zfs_url}[ZFS]

. Shared Filesystems
* {cephfs_url}[CephFS]
* {nfs_url}[NFS]
